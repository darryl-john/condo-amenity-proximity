{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cd2167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import time, random, requests, json, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3e3441",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = requests.Session()\n",
    "\n",
    "# Set headers for the session\n",
    "s.headers.update({\n",
    "   \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                 \"(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n",
    "   \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "   \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "})\n",
    "\n",
    "# Function to fetch a URL with retries\n",
    "def fetch(url, max_tries=10):\n",
    "   delay = 3.0\n",
    "   for i in range(max_tries):\n",
    "       r = s.get(url, timeout=60)\n",
    "       if r.status_code in (200, 304):\n",
    "           # polite delay between successful fetches\n",
    "           time.sleep(delay + random.random()*2)\n",
    "           return r\n",
    "       if r.status_code in (429, 503):  # too many / temporarily blocked\n",
    "           time.sleep(delay)\n",
    "           delay *= 2\n",
    "           continue\n",
    "       r.raise_for_status()\n",
    "   raise RuntimeError(f\"Failed after {max_tries} tries: {url}\")\n",
    "\n",
    "# Function to extract string from a BeautifulSoup object\n",
    "def get_text(object, tag, attributes, text=None):\n",
    "    try:\n",
    "        return object.find(tag, attrs=attributes, text=text).string.strip()\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Function for error handling\n",
    "def maybe(function):\n",
    "    try:\n",
    "        return function()\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c04279",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.lamudi.com.ph\"\n",
    "\n",
    "# Condo For Sale in Metro Manila with Price above 1 peso and Floor Area above 1 sqm\n",
    "list_url = \"https://www.lamudi.com.ph/buy/pasig/quezon-city/condo/?foreclosures=excluded&min-price=1&minArea=1&sorting=newest\"\n",
    "\n",
    "start_page = 41\n",
    "end_page = 50\n",
    "\n",
    "# Lamudi only allows navigation for upto 50 pages (with 30 listings per page) for a total of 3000 listings.\n",
    "# Scraping methodology: Scrape 50 pages per city, in batches of 10 to avoid overloading their servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7024fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or clear the output file\n",
    "with open('lamudi-listings.txt', 'w', encoding=\"utf-8\") as f:\n",
    "    pass\n",
    "\n",
    "results = []\n",
    "batch = []\n",
    "batch_size = 50\n",
    "\n",
    "# Fetch listings from the specified pages and write to file\n",
    "with open('lamudi-listings.txt', 'a', encoding=\"utf-8\") as f:\n",
    "    for page in range(start_page, end_page + 1):\n",
    "        response = fetch(list_url + ((\"&page=\" + str(page)) if page > 1 else \"\" ))\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        listings = soup.find_all('div', class_=\"snippet__content\")\n",
    "\n",
    "        for listing in listings:\n",
    "\n",
    "            response = fetch(base_url + listing.find('a', href=True)['href'])\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            details = soup.find('div', class_=\"adform__detail\")\n",
    "\n",
    "            # Extract essential details from the listing\n",
    "            title = get_text(details, 'div', {\"class\": \"main-title\"})\n",
    "            description = get_text(details, 'div', {\"id\": \"description-text\", \"class\": \"content\"})\n",
    "            project_unit = get_text(details, 'a', {\"id\": \"project-unit__title\", \"class\": \"link\"})\n",
    "            project_name = get_text(details, 'span', {\"class\": \"place-features__values\", \"data-test\": \"project-name-value\"})\n",
    "            price = get_text(details, 'div', {\"class\": \"prices-and-fees__price\"})\n",
    "            location = get_text(details, 'div', {\"class\": \"view-map__text\"})\n",
    "            bedrooms = get_text(details, 'div', {\"class\": \"details-item-value\", \"data-test\": \"bedrooms-value\"})\n",
    "            bathrooms = get_text(details, 'div', {\"class\": \"details-item-value\", \"data-test\": \"full-bathrooms-value\"})\n",
    "            area = get_text(details, 'div', {\"class\": \"details-item-value\", \"data-test\": \"area-value\"})\n",
    "            floor_area = get_text(details, 'span', {\"class\": \"place-features__values\", \"data-test\": \"floor-area-value\"})\n",
    "            floor = get_text(details, 'span', {\"class\": \"place-features__values\", \"data-test\": \"floor-value\"})\n",
    "            condition = get_text(details, 'div', {\"class\": \"facilities__item\"}, text=re.compile(r'.*?furnished'))\n",
    "            property_type = get_text(details, 'span', {\"class\": \"place-features__values\", \"data-test\": \"property-type-value\"})\n",
    "            offer_type = get_text(details, 'span', {\"class\": \"place-features__values\", \"data-test\": \"operation-type-value\"})\n",
    "            construction_year = get_text(details, 'span', {\"class\": \"place-features__values\", \"data-test\": \"construction-year-value\"})\n",
    "            parking_spaces = get_text(details, 'span', {\"class\": \"place-features__values\", \"data-test\": \"parking-spaces-value\"})\n",
    "            ownership_type = get_text(details, 'span', {\"class\": \"place-features__values\", \"data-test\": \"ownership-value\"})\n",
    "            publish_date = get_text(details, 'div', {\"class\": \"date\"})\n",
    "            project_link = maybe(lambda: details.find('a', {\"class\": \"detail-page-project__link\"}, href=True)['href'])\n",
    "\n",
    "            # Do not include listings with missing essential details\n",
    "            # if price == None or floor_area == None or location == None or project_name == None:\n",
    "            #    continue\n",
    "\n",
    "            record = {\n",
    "                'title': title,\n",
    "                'description': description,\n",
    "                'project_unit' : project_unit,\n",
    "                'project_name': project_name,\n",
    "                'area' : area,\n",
    "                'floor_area': floor_area,\n",
    "                'price': price,\n",
    "                'location': location,\n",
    "                'bedrooms': bedrooms,\n",
    "                'bathrooms': bathrooms,\n",
    "                'floor': floor,\n",
    "                'condition': condition,\n",
    "                'property_type': property_type,\n",
    "                'offer_type': offer_type,\n",
    "                'construction_year': construction_year,\n",
    "                'parking_spaces': parking_spaces,\n",
    "                'ownership_type': ownership_type,\n",
    "                'project_link': project_link,\n",
    "                'publish_date': maybe(lambda: publish_date.split(\" - Published by \")[0]),\n",
    "                'publish_by':  maybe(lambda: publish_date.split(\" - Published by \")[1])\n",
    "            }\n",
    "\n",
    "            # Skip records with all None values\n",
    "            if all(value is None for value in record.values()):\n",
    "                continue\n",
    "            \n",
    "            # Convert record to JSON line\n",
    "            json_line = json.dumps(record, ensure_ascii=False)\n",
    "\n",
    "            # Add to results for DataFrame\n",
    "            results.append(record)\n",
    "\n",
    "            # Add to batch for file writing\n",
    "            batch.append(json_line)\n",
    "\n",
    "            # Save to file every batch_size items\n",
    "            if len(batch) == batch_size or page == end_page:\n",
    "                f.write('\\n'.join(batch) + '\\n')\n",
    "                batch = []\n",
    "            \n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
